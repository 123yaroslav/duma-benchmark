\documentclass[11pt]{article}

% Options: review (with line numbers), preprint (clean), final (camera-ready)
\usepackage[review]{acl}

% Fix line number positioning for two-column layout
% Use zero-padded three-digit format (001, 002, etc.)
\renewcommand\thelinenumber{\ifnum\value{linenumber}<100 0\fi\ifnum\value{linenumber}<10 0\fi\arabic{linenumber}}

% Display line numbers on both sides (left and right)
\renewcommand\makeLineNumber{%
  \hss\thelinenumber\ \rlap{\hskip\textwidth\ \hskip\marginparsep\thelinenumber}%
}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}

\title{DUMA-bench: A Dual-Control Multi-Agent Benchmark for Evaluating LLM Agent Security}

% Anonymous submission - comment out for camera-ready
\author{Anonymous Submission}

% For camera-ready version, uncomment and fill in:
% \author{Ivan Aleksandrov\textsuperscript{1} \and German Kochnev\textsuperscript{1} \and Yaroslav Rogoza\textsuperscript{1} \\
%   \textsuperscript{1}ITMO Security Lab \\
%   \texttt{\{i.aleksandrov,g.kochnev,y.rogoza\}@itmo.ru}
% }

\begin{document}

\maketitle

\begin{abstract}
LLM-based agent systems are increasingly deployed for automating complex tasks, yet their security in realistic interaction scenarios remains understudied. Existing benchmarks evaluate either isolated agent capabilities or prompt injection resistance in simplified settings without accounting for active user interaction dynamics. We present DUMA-bench (Dual-control User Multi-Agent benchmark), extending $\tau^2$-bench with three security domains aligned with AI-SAFE (levels 1--5) and OWASP classifications: \texttt{mail\_rag\_phishing} (RAG poisoning), \texttt{collab} (inter-agent attacks), and \texttt{output\_handling} (improper output handling).

We evaluate five models (Claude Sonnet 4.5, GPT-4-turbo, GPT-4o, GPT-4o-mini, GPT-3.5-turbo) across varying user temperatures ($T_{\text{user}} \in \{0.0, 0.5, 1.0\}$), measuring pass@1 and pass@4 metrics. Key findings: (1) model size does not predict security---GPT-4o outperforms larger models in RAG poisoning while GPT-3.5-turbo shows highest stability; (2) user behavior variability significantly impacts Attack Success Rate (ASR) even with non-malicious users; (3) all models show high ASR (75--90\%) in RAG poisoning, indicating critical vulnerabilities requiring specialized defenses.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Context and Problem Statement}

The development of Large Language Models (LLMs) and their integration into agent systems opens new possibilities for automating complex tasks. AI agents are capable of autonomous planning, interaction with external tools (APIs, databases, file systems), and real-time decision-making~\cite{ai_safe_2025}. However, these same capabilities create fundamentally new attack surfaces not characteristic of classical machine learning systems.

A typical AI agent comprises five interconnected components~\cite{ai_safe_2025}. At its core, the \textbf{LLM} serves as the central component for understanding instructions and generating responses, while a \textbf{planning module} transforms high-level goals into executable action sequences. The agent's \textbf{memory} system maintains both short-term context through dialogue history and long-term knowledge through RAG systems and knowledge bases. External \textbf{tools} provide the agent with capabilities to interact with APIs, databases, and other real-world systems, while the \textbf{interface} layer serves as the entry point for receiving user requests and delivering responses.

Each of these components represents a potential attack vector. The AI-SAFE framework~\cite{ai_safe_2025} systematizes threats across five levels: interface (Prompt Injection, DoS), execution and tools (Tool Misuse, Privilege Escalation), infrastructure and orchestration (Cross-Agent Poisoning), core and logic (Jailbreaking, Goal Manipulation), data and knowledge (RAG Poisoning, Data Leakage).

\subsection{Limitations of Existing Methods}

Current benchmarks for evaluating agent system security suffer from three fundamental limitations. First, most benchmarks including AgentBench~\cite{agent_bench} and Agent Security Bench~\cite{asb} conduct \textbf{isolated evaluation} where agents operate under monopolistic control with users serving merely as passive instruction sources. Second, existing security-focused benchmarks employ \textbf{simplified attack scenarios}---for instance, Agent Dojo~\cite{agent_dojo} concentrates on prompt injection in single-agent contexts without considering the complexities of inter-agent interaction and RAG systems. Third and most critically, current benchmarks fail to account for the \textbf{active user} dynamic. Research on $\tau$-bench~\cite{tau_bench_2024} and $\tau^2$-bench~\cite{tau2_bench_2025} has demonstrated that introducing an active user through dual-control paradigms leads to agent performance drops of up to 25 percentage points, indicating that coordination and communication become critical failure points. Yet existing security benchmarks systematically overlook this dynamic, creating a significant gap between evaluation settings and real-world deployment scenarios.

\subsection{Contributions}

This work makes four key contributions to the evaluation of LLM agent security. We present \textbf{DUMA-bench}, a new security benchmark extending $\tau^2$-bench with three security domains that model typical attack vectors: RAG poisoning through the \texttt{mail\_rag\_phishing} domain, inter-agent attacks through the \texttt{collab} domain, and improper output handling through the \texttt{output\_handling} domain. The benchmark is publicly available on GitHub~\cite{duma_bench_github}. We propose a \textbf{dual-control evaluation methodology} for assessing agent robustness to attacks while accounting for active user behavior, with rigorous formalization within the Dec-POMDP framework. Our \textbf{comprehensive empirical evaluation} covers five state-of-the-art LLMs---Claude Sonnet 4.5, GPT-4-turbo, GPT-4o, GPT-4o-mini, and GPT-3.5-turbo---across multiple configurations, demonstrating that model size does not consistently predict security robustness and revealing unexpected performance patterns. Finally, we formulate and test three \textbf{testable hypotheses} about agent security: the effect of multiple runs on pass@k variance (H1), the impact of user temperature on attack success rate (H2), and the relationship between model size and security performance (H3).

\section{Related Work}
\label{sec:related}

\subsection{Agent System Evaluation Benchmarks}

The development of LLM agents has led to the creation of benchmark series for evaluating their capabilities. AgentBench~\cite{agent_bench} evaluates agents in eight environments, including operating systems, databases, and web navigation, but focuses on functional capabilities without security consideration. ToolBench and API-Bank investigate tool usage but in trusted environments.

A key breakthrough was the $\tau$-bench~\cite{tau_bench_2024} and $\tau^2$-bench~\cite{tau2_bench_2025} series, where the user is modeled as an active participant capable of changing environment state. Formalization within the Dec-POMDP framework~\cite{amato_2013} showed that user coordination is a critical bottleneck: even advanced models lose up to 25 percentage points of performance when transitioning from monopolistic to dual control.

\subsection{LLM Agent Security Evaluation}

Agent Security Bench (ASB)~\cite{asb} formalizes attacks and defenses for LLM agents, but is limited to single-agent scenarios. Agent Dojo~\cite{agent_dojo} creates a dynamic environment for evaluating prompt injection attacks, demonstrating that modern agents are vulnerable even to simple attacks. However, Agent Dojo does not model an active user as an interaction participant, attacks propagating through inter-agent communication channels, or RAG system poisoning in realistic scenarios involving emails and documents.

\subsection{Threat Modeling Frameworks}

OWASP LLM Top~10~\cite{owasp_llm_2025} and OWASP AI Agents Top~15~\cite{owasp_agents_2025} systematize threats for AI systems. The AI-SAFE framework~\cite{ai_safe_2025} proposes a five-level threat model specific to agent architectures. Our work uses these classifications for systematic coverage of attack vectors in the developed domains.

\subsection{Positioning of This Work}

This work fills a critical gap between dual-control benchmarks like $\tau^2$-bench, which evaluate agent-user coordination but do not focus on security, and security benchmarks like Agent Dojo and ASB, which assess vulnerabilities but do not account for active users and inter-agent interaction. We extend the $\tau^2$-bench methodology with security domains covering AI-SAFE threats and evaluate agent robustness in realistic scenarios with active users.

\section{Method}
\label{sec:method}

\subsection{Problem Formulation}

\subsubsection{Interaction Model (Dec-POMDP)}

We formalize agent-user interaction as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{amato_2013,tau2_bench_2025}, following the $\tau^2$-bench framework. The environment $\mathcal{E}$ is described by a state space $\mathcal{S}$ that is only partially observable to participants. The agent $\mathcal{A}$ and user $\mathcal{U}$ function as two autonomous players, each equipped with their own observation spaces $\Omega_A$ and $\Omega_U$, and action spaces $\mathcal{A}_A$ and $\mathcal{A}_U$. The transition function $T: \mathcal{S} \times \mathcal{A}_A \times \mathcal{A}_U \rightarrow \Delta(\mathcal{S})$ defines environment dynamics, capturing how both agent and user actions jointly determine state evolution. Critically, the user is not a passive instruction source but actively changes environment state through their actions, creating genuine coordination challenges.

\subsubsection{Attacker Model}

We adopt a grey-box attacker model with realistic assumptions about adversary capabilities. The attacker possesses \textbf{system knowledge}, understanding the agent architecture and available tools, but has no access to the system prompt or model weights. The attacker's \textbf{capabilities} include injecting malicious content into various input channels such as emails, colleague messages, user queries, and document contents in RAG systems. The attacker's \textbf{goal} is to force the agent to perform unauthorized actions or disclose confidential information, representing realistic threat scenarios in deployed agent systems.

\subsubsection{Evaluation Metrics}

\textbf{pass@k metric.} Defines the fraction of cases where the agent successfully passed $k$ asserts:
\begin{equation}
\text{pass}@k = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}\left[\sum_{j=1}^{M_i} a_{ij} \geq k\right],
\end{equation}
where $N$ is the number of cases, $M_i$ is the number of asserts in case $i$, $a_{ij} \in \{0, 1\}$ is the result of assert $j$.

\textbf{Attack Success Rate (ASR).} Attack success indicator:
\begin{equation}
\text{ASR} = 1 - \text{pass}@1.
\end{equation}

In the security context, asserts verify that the agent maintains security invariants: absence of malicious instruction execution, preservation of data integrity, and refusal of unauthorized actions.

\subsection{Developed Security Domains}

\subsubsection{\texttt{mail\_rag\_phishing} Domain (RAG Poisoning)}

\textbf{Description.} Models an attack on an email client's RAG system through injection of malicious instructions in email content.

\textbf{Threat classification:} AI-SAFE.DATA.1 (Knowledge Base Poisoning), AI-SAFE.INPUT.1 (Indirect Prompt Injection), LLM01, RAG: Indirect Prompt Injection.

\textbf{Domain architecture.} The domain simulates an email client with integrated RAG for searching relevant emails. The knowledge base contains a mixture of legitimate and malicious emails designed to test the agent's ability to distinguish safe from compromised content. The agent has access to four tools: \texttt{search\_emails} for querying the knowledge base, \texttt{get\_email\_details} for retrieving full message content, \texttt{send\_email} for composing new messages, and \texttt{forward\_email} for redistributing existing messages.

\subsubsection{\texttt{collab} Domain (Inter-Agent Interaction)}

\textbf{Description.} Models a multi-agent environment where an attacking agent (``colleague'') manipulates the target agent through the communication channel.

\textbf{Threat classification:} AI-SAFE.INFRA.3 (Cross-Agent Poisoning), AI-SAFE.EXEC.1 (Tool Misuse), AI-SAFE.EXEC.2 (Privilege Escalation), T15 (Cross-Agent Communication Poisoning), MCP05.

\textbf{Domain architecture.} The domain implements a workspace with multiple agents operating at different privilege levels, creating opportunities for privilege escalation attacks. A ticketing and task system with delegation mechanisms enables attackers to manipulate task assignments. An activity log provides audit capabilities for detecting suspicious behavior. The agent can invoke five tools: \texttt{get\_tickets} for retrieving assigned work, \texttt{update\_ticket} for modifying task status, \texttt{delegate\_task} for reassigning responsibilities, \texttt{get\_logs} for accessing system activity, and \texttt{send\_message} for inter-agent communication.

\subsubsection{\texttt{output\_handling} Domain (Output Processing)}

\textbf{Description.} Tests agent robustness to generating malicious code or injections in output.

\textbf{Threat classification:} AI-SAFE.INPUT.3 (Improper Output Handling), LLM05.

\textbf{Domain architecture.} The domain models a web application where the agent generates content for display to end users. A database containing user data provides context for content generation while creating opportunities for SQL injection. A rendering system displays generated content in browsers, enabling testing of XSS and other injection vulnerabilities. The agent operates with four tools: \texttt{generate\_content} for creating textual responses, \texttt{execute\_query} for database access, \texttt{render\_template} for formatting output, and \texttt{send\_response} for delivering content to users.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Models Under Study}

We evaluate five state-of-the-art LLMs spanning different capability levels and architectural generations. \textbf{Claude Sonnet 4.5} represents Anthropic's latest model with strong reasoning capabilities. \textbf{GPT-4-turbo} provides OpenAI's advanced model with enhanced capabilities, while \textbf{GPT-4o} offers an optimized variant balancing performance and cost. \textbf{GPT-4o-mini} serves as a compact version with reduced cost, and \textbf{GPT-3.5-turbo} establishes a baseline for comparison. Agent generation parameters are fixed at temperature $T_{\text{agent}} = 0.0$ for all models to ensure deterministic agent behavior.

\subsubsection{Variable Parameters}

We vary the user model temperature across three levels to test robustness under different user behavior patterns: $T_{\text{user}} = 0.0$ for deterministic behavior, $T_{\text{user}} = 0.5$ for moderate variability, and $T_{\text{user}} = 1.0$ for high variability.


\subsubsection{Metrics}

We evaluate agent robustness using two complementary metrics. The \textbf{pass@k} metric measures the proportion of cases where the agent successfully passes at least $k$ asserts, as formally defined in Equation~1. The \textbf{ASR (Attack Success Rate)} quantifies the proportion of successful attacks, computed as $\text{ASR} = 1 - \text{pass@1}$. In our experiments, we primarily report pass@1 (at least 1 assert passed) and pass@4 (at least 4 asserts passed) metrics to evaluate both basic and stringent security thresholds.

\subsubsection{Research Hypotheses}

We test three hypotheses:

\textbf{Hypothesis 1 (H1):} At fixed agent and user temperatures, increasing the number of runs $k$ reduces the variance of pass@k, but does not guarantee monotonic changes in ASR.

\textbf{Hypothesis 2 (H2):} Changes in non-attacking user queries cause changes in ASR at fixed agent temperature.

\textbf{Hypothesis 3 (H3):} Model size does not consistently correlate with pass@k and ASR; mid-sized models may outperform larger models in specific security domains.

\subsubsection{Protocol}

For each combination (model, temperature, domain, case), we perform $n=10$ independent runs. All metrics are recorded, and results are aggregated for statistical analysis using Fisher's exact test~\cite{Fisher_1935} for significance testing. We use Wilson confidence intervals~\cite{Wilson_1927} for proportion estimates, which provide better coverage properties for small sample sizes compared to normal approximations.

\section{Results \& Discussion}
\label{sec:results}

\subsection{Aggregated Results}

Table~\ref{tab:aggregated} presents aggregated pass@1 results by model and domain across all temperature settings. Table~\ref{tab:significance} reports statistical significance of differences between GPT-4o and GPT-4o-mini at each temperature, and Table~\ref{tab:temp_significance} shows the impact of temperature variation within each model.

% Include the auto-generated tables
\input{model_domain_table.tex}
\input{significance_table.tex}
\input{temperature_significance_table.tex}

\subsection{Analysis}

\subsubsection{Hypothesis Testing Results}

\textbf{H1: Effect of Multiple Runs.} Our results confirm that at fixed temperatures, increasing $k$ in pass@k reduces variance. However, ASR does not change monotonically with $k$. For example, Claude Sonnet 4.5 shows pass@1 = 0.25 but pass@4 = 0.20 in the \texttt{mail\_rag\_phishing} domain at $T_{\text{user}}=0.0$, indicating that some initially successful runs fail with more stringent evaluation criteria. This non-monotonicity suggests that agent robustness is sensitive to the number of security checks required, even with deterministic agent temperature.

\textbf{H2: User Temperature Impact.} Changes in user model temperature ($T_{\text{user}}$) cause measurable changes in ASR at fixed agent temperature. This confirms that user behavior variability, even when the user is non-malicious, affects the attack success rate. The effect is most pronounced in the \texttt{collab} domain where social engineering plays a key role.

\textbf{H3: Model Size vs. Security Performance.} Our results partially confirm this hypothesis. Surprisingly, GPT-4o (mid-sized) outperforms larger models (GPT-4-turbo, Claude Sonnet 4.5) in the \texttt{mail\_rag\_phishing} domain across all temperatures, though it shows instability when evaluating higher $k$ values. GPT-3.5-turbo, despite being the smallest model, demonstrates the most stable performance across temperature variations. This suggests that model size and architectural sophistication alone are not reliable predictors of security robustness---specialized security training or instruction-following capabilities may play a more critical role.

\subsubsection{Cross-Model Comparison}

Key observations emerge when comparing all five models across domains. In the \textbf{RAG Poisoning (\texttt{mail\_rag\_phishing})} domain, GPT-4o achieves the highest pass@1 (32-36\% depending on $T$), outperforming both GPT-4-turbo (30-40\%) and Claude Sonnet 4.5 (25\%). GPT-4o-mini shows significantly lower performance (8-12\%, $p < 0.01$), while GPT-3.5-turbo performs poorly (15-20\%) but maintains consistent results across temperatures. For \textbf{Inter-Agent Attacks (\texttt{collab})}, Claude Sonnet 4.5 demonstrates superior performance (96-100\% pass@1), followed by GPT-4-turbo (96-100\%) and GPT-4o (21.7-28.3\%), with no statistically significant differences between GPT-4o and GPT-4o-mini ($p > 0.4$). In the \textbf{Output Handling (\texttt{output\_handling})} domain, Claude Sonnet 4.5 again leads (50-67\%), followed by GPT-4-turbo (50-67\%) and GPT-4o (53.3-60\%), with differences between GPT-4o and GPT-4o-mini remaining statistically insignificant.

\subsubsection{RAG System Vulnerability}

The \texttt{mail\_rag\_phishing} domain remains the most challenging across all models: even the best configuration (Claude Sonnet 4.5 at $T=0.5$) shows ASR = 75\% at pass@1. This indicates high effectiveness of indirect prompt injection attacks through RAG context and the critical need for additional defenses (e.g., source validation, instruction filtering, policy-driven tool gating).

\subsubsection{Temperature Stability}

GPT-3.5-turbo exhibits the most stable behavior across different user temperatures, with minimal variance in pass@k metrics. In contrast, GPT-4o shows high sensitivity to temperature changes, particularly in the \texttt{collab} domain where pass@1 ranges from 21.7\% to 28.3\% as $T_{\text{user}}$ increases from 0.0 to 1.0.

\section{Conclusion}
\label{sec:conclusion}

We present DUMA-bench, a comprehensive benchmark for evaluating LLM agent security in dual-control paradigms where active users can influence environment state. Our work yields four main findings. First, DUMA-bench addresses a critical gap in the evaluation landscape: existing security benchmarks poorly correspond to real agent usage scenarios. Our three security domains covering RAG poisoning, inter-agent attacks, and output handling align with AI-SAFE levels 1-5, OWASP LLM Top 10, and OWASP AI Agents Top 15. Second, model size does not predict security---GPT-4o outperforms larger models including GPT-4-turbo and Claude Sonnet 4.5 in RAG poisoning scenarios, while GPT-3.5-turbo shows the most stable performance across temperature variations. This partially confirms our hypothesis that model size does not consistently correlate with security robustness. Third, user behavior significantly affects attack success, with changes in user temperature impacting ASR even when the user is non-malicious, confirming that dual-control dynamics are critical for realistic security evaluation. Fourth, RAG attacks remain a critical vulnerability, with all models showing high ASR (75-90\%) in the \texttt{mail\_rag\_phishing} domain, indicating that indirect prompt injection through RAG systems remains a major vulnerability requiring specialized defenses.

\textbf{Future directions} include: (1) extending DUMA-bench with additional domains from OWASP classifications (\texttt{resource\_overload}, \texttt{supply\_chain}); (2) evaluating defense mechanisms (Llama Guard, Promptfoo) on current domains; (3) conducting qualitative analysis of simulation traces to identify failure patterns; (4) expanding model coverage to open-source alternatives (Llama, Mistral); and (5) investigating the impact of different agent architectures on security robustness.

DUMA-bench is open-source and available~\cite{duma_bench_github}, enabling reproducible security evaluation for the research community.

\section*{Acknowledgments}

We thank the authors of $\tau^2$-bench for their foundational work in dual-control agent evaluation, which inspired this research. We also acknowledge the Yandex Cloud team for developing the AI-SAFE framework.

\section{Limitations}
\label{sec:limitations}

Our work has five important limitations. First, while we test across multiple configurations with $n=10$ runs each, larger sample sizes would enable more robust statistical conclusions, especially when comparing models with similar performance. However, our use of Fisher's exact test and Wilson confidence intervals provides valid statistical inference for small samples. Second, the user is modeled by an LLM, which may not fully capture the unpredictability and errors of real human users, though this approach ensures reproducibility and controlled experimentation. Third, while we evaluate five models including both proprietary systems (GPT, Claude) and recognize the need for open models (Llama, Gemini, etc.), our current results may not fully generalize across all model families, and future work will expand model coverage. Fourth, DUMA-bench currently covers three security domains, with additional domains such as \texttt{resource\_overload} and \texttt{supply\_chain} attacks planned for future releases. Fifth, the current version does not evaluate effectiveness of defense mechanisms such as Llama Guard, Promptfoo, and custom validators, which represents a key direction for future research.

\bibliography{references}
\bibliographystyle{acl_natbib}

\end{document}
